{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Build-Vocabulary\" data-toc-modified-id=\"Build-Vocabulary-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Build Vocabulary</a></span></li><li><span><a href=\"#Unigram-spell-checker\" data-toc-modified-id=\"Unigram-spell-checker-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Unigram spell checker</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluate-accuracy\" data-toc-modified-id=\"Evaluate-accuracy-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Evaluate accuracy</a></span></li></ul></li><li><span><a href=\"#Bigram-spell-checker\" data-toc-modified-id=\"Bigram-spell-checker-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Bigram spell checker</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluating\" data-toc-modified-id=\"Evaluating-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Evaluating</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:09:12.835400Z",
     "start_time": "2022-01-25T10:09:12.815472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:09:36.171274Z",
     "start_time": "2022-01-25T10:09:36.150590Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import os,sys,inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir) \n",
    "sys.path.insert(0, '../') \n",
    "\n",
    "\n",
    "import data\n",
    "from data import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:09:36.538231Z",
     "start_time": "2022-01-25T10:09:36.340010Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = utils.get_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:09:36.929736Z",
     "start_time": "2022-01-25T10:09:36.905266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': ['they', 'can', 'go', 'quite', 'farst'],\n",
       " 'corrected': ['they', 'can', 'go', 'quite', 'fast'],\n",
       " 'indexes': [4]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:09:37.397222Z",
     "start_time": "2022-01-25T10:09:37.375259Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tr = [x['original'] for x in train]\n",
    "y_tr = [x['corrected'] for x in train]\n",
    "y_tr_idx =  [x['indexes'] for x in train]\n",
    "\n",
    "X_te = [x['original'] for x in test]\n",
    "y_te = [x['corrected'] for x in test]\n",
    "y_te_idx =  [x['indexes'] for x in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:09:37.625954Z",
     "start_time": "2022-01-25T10:09:37.573107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vocabulary)=|3451\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "\n",
    "bigram_finder = BigramCollocationFinder.from_documents(X_tr)\n",
    "bigram_freq_dict = dict(bigram_finder.ngram_fd.items())\n",
    "list(bigram_freq_dict.keys())[0:10]\n",
    "\n",
    "import itertools\n",
    "vocabulary = set(list(itertools.chain(*bigram_freq_dict.keys())))\n",
    "print(f'len(vocabulary)=|{len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:09:37.845945Z",
     "start_time": "2022-01-25T10:09:37.824645Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_freq_dict = dict(bigram_finder.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:09:38.833691Z",
     "start_time": "2022-01-25T10:09:38.815524Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:09:39.041807Z",
     "start_time": "2022-01-25T10:09:39.021462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'man', 'went'), ('man', 'went', 'to'), ('went', 'to', 'the')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in nltk.collocations.ngrams(['the','man','went','to','the'],n=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:09:39.919455Z",
     "start_time": "2022-01-25T10:09:39.896806Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "corrected_sentences = [train[n]['corrected'] for n in range(len(train))]\n",
    "corrected_words = [word.lower() for sentence in corrected_sentences for word in sentence]\n",
    "unique_corrected_words = set(corrected_words)\n",
    "n_total_words = len(corrected_words)\n",
    "vocabulary = unique_corrected_words\n",
    "\n",
    "def build_unigrams(corrected_words):\n",
    "    return Counter(corrected_words) \n",
    "\n",
    "def prob(word, unigrams, n_total_words):\n",
    "    word = word.lower()\n",
    "    word_counts = unigrams[word]\n",
    "    return word_counts / n_total_words\n",
    "\n",
    "# Test your code with the following\n",
    "# assert(unigram(\"me\")==87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:09:40.748026Z",
     "start_time": "2022-01-25T10:09:40.727617Z"
    }
   },
   "outputs": [],
   "source": [
    "unigrams = build_unigrams(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:09:40.974667Z",
     "start_time": "2022-01-25T10:09:40.955135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002200926223118896\n",
      "0.003989178779402999\n",
      "0.00018341051859324133\n"
     ]
    }
   ],
   "source": [
    "print(prob('house', unigrams, n_total_words))\n",
    "print(prob('me', unigrams, n_total_words))\n",
    "print(prob('television', unigrams, n_total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:09:41.602273Z",
     "start_time": "2022-01-25T10:09:41.582764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 87, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams['house'], unigrams['me'], unigrams['television']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:09:42.197850Z",
     "start_time": "2022-01-25T10:09:42.179470Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:09:42.794067Z",
     "start_time": "2022-01-25T10:09:42.775051Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_candidates(token, vocabulary):\n",
    "    # Write your code here.\n",
    "    distance_token_to_words = {word:edit_distance(word,token.lower()) for word in vocabulary}\n",
    "    minimum_distance = min(distance_token_to_words.values())\n",
    "    return sorted([word for word, distance in distance_token_to_words.items() \n",
    "                   if distance <= minimum_distance], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:09:43.468539Z",
     "start_time": "2022-01-25T10:09:43.450296Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test your code as follows\n",
    "#assert get_candidates(\"minde\",vocabulary) == ['mine', 'mind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:09:44.883813Z",
     "start_time": "2022-01-25T10:09:44.823087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boss', 'box', 'boys']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidates(\"boxs\",vocabulary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T13:41:38.321716Z",
     "start_time": "2021-04-22T13:41:35.495079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.6 ms ± 270 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit get_candidates(\"min\",vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram spell checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:09.886866Z",
     "start_time": "2022-01-25T10:10:09.866492Z"
    }
   },
   "outputs": [],
   "source": [
    "def correct_tokens(tokenized_sentence, unigrams, n_total_words):\n",
    "    tokenized_sentence = tokenized_sentence.copy()\n",
    "    \n",
    "    for index,word in enumerate(tokenized_sentence):\n",
    "        if (word and word.lower()) not in unique_corrected_words:\n",
    "            candidates = {candidate:prob(candidate, unigrams, n_total_words) for candidate in get_candidates(word,vocabulary)}\n",
    "            best_candidate  = max(candidates, key=candidates.get)\n",
    "            tokenized_sentence[index] = best_candidate\n",
    "\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:10.318942Z",
     "start_time": "2022-01-25T10:10:10.249056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'white', 'cat']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_tokens(tokenized_sentence = [\"this\", \"whitr\", \"cat\"],  \n",
    "               unigrams=unigrams, \n",
    "               n_total_words=n_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:10.513163Z",
     "start_time": "2022-01-25T10:10:10.451322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'my', 'cat']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_tokens(tokenized_sentence = [\"this\",\"is\",\"my\",\"caat\"],  \n",
    "               unigrams=unigrams, \n",
    "               n_total_words=n_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:10.730679Z",
     "start_time": "2022-01-25T10:10:10.658347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Joan', 'likes', 'pigs', 'and', 'cats']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_tokens(tokenized_sentence = [\"Joan\",\"likes\",\"pissa\",\"and\",\"cats\"],  \n",
    "               unigrams=unigrams, \n",
    "               n_total_words=n_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:10.880419Z",
     "start_time": "2022-01-25T10:10:10.859998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'pizza' in vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:12.023493Z",
     "start_time": "2022-01-25T10:10:12.003514Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(test,  unigrams, n_total_words):\n",
    "    # Write your code here\n",
    "    count_total_words = 0\n",
    "    count_corrected_words = 0\n",
    "    for sentence in test:\n",
    "        corrected_sentence = correct_tokens(sentence['original'], unigrams, n_total_words)  \n",
    "        count_total_words +=len(sentence['corrected'])\n",
    "        count_corrected_words += sum(corrected_sentence[n] == sentence['corrected'][n] \n",
    "                                     for n in range(len(sentence['corrected'])))\n",
    "    return count_corrected_words/count_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:22.125854Z",
     "start_time": "2022-01-25T10:10:12.250819Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8397887323943662"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(test, unigrams, n_total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram spell checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:22.267571Z",
     "start_time": "2022-01-25T10:10:22.213107Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from collections import Counter\n",
    "\n",
    "corrected_sentences = [train[n]['corrected'] for n in range(len(train))]\n",
    "corrected_words = [word.lower() for sentence in corrected_sentences for word in sentence]\n",
    "unique_corrected_words = set(corrected_words)\n",
    "finder = BigramCollocationFinder.from_words(corrected_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:22.371394Z",
     "start_time": "2022-01-25T10:10:22.351206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they', 'can', 'go', 'quite', 'fast', 'this', 'was', 'a', 'royal', 'enfield']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:22.481372Z",
     "start_time": "2022-01-25T10:10:22.459262Z"
    }
   },
   "outputs": [],
   "source": [
    "unigram_freq_dict = build_unigrams(corrected_words)\n",
    "bigram_freq_dict  = dict(finder.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:22.591442Z",
     "start_time": "2022-01-25T10:10:22.569022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they', 'can', 'go', 'quite', 'fast', 'this', 'was', 'a', 'royal', 'enfield']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:22.704709Z",
     "start_time": "2022-01-25T10:10:22.681548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('they', 'can'), 4),\n",
       " (('can', 'go'), 5),\n",
       " (('go', 'quite'), 1),\n",
       " (('quite', 'fast'), 1),\n",
       " (('fast', 'this'), 1),\n",
       " (('this', 'was'), 4),\n",
       " (('was', 'a'), 35),\n",
       " (('a', 'royal'), 3),\n",
       " (('royal', 'enfield'), 5),\n",
       " (('enfield', '_'), 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools \n",
    "def top_k_dict(d, top_k = 10):\n",
    "    return [(x,bigram_freq_dict[x]) for k,x in enumerate(bigram_freq_dict) if k<top_k]\n",
    "\n",
    "top_k_dict(bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can notice that actually the bigrams where not computed correctly because we concatenated all the data from the  corrected sentences into a single list `corrected_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:22.818157Z",
     "start_time": "2022-01-25T10:10:22.797437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': ['they', 'can', 'go', 'quite', 'farst'],\n",
       " 'corrected': ['they', 'can', 'go', 'quite', 'fast'],\n",
       " 'indexes': [4]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:22.927386Z",
     "start_time": "2022-01-25T10:10:22.905576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': ['this', 'was', 'a', 'Royl', 'Enfield', 'Consulatoin', '?', '_'],\n",
       " 'corrected': ['this', 'was', 'a', 'Royal', 'Enfield', '_', '?', '_'],\n",
       " 'indexes': [3, 5]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how the bigram `(fast,this)` appears in the `bigram_freq_dict` but in reality this might not even be in the training data.\n",
    "\n",
    "It is a consequence of concatenating the training dat into a single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:23.038436Z",
     "start_time": "2022-01-25T10:10:23.016770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('they', 'can'),\n",
       " ('can', 'go'),\n",
       " ('go', 'quite'),\n",
       " ('quite', 'fast'),\n",
       " ('fast', 'this'),\n",
       " ('this', 'was')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigram_freq_dict)[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could build the bigrams avoiding this issue using `BigramCollocationFinder.from_documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:24.658197Z",
     "start_time": "2022-01-25T10:10:24.637880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(('they', 'can'), 1), (('can', 'go'), 1), (('go', 'quite'), 1), (('quite', 'fast'), 1), (('this', 'was'), 1), (('was', 'a'), 1), (('a', 'Royal'), 1), (('Royal', 'Enfield'), 1), (('Enfield', '_'), 1), (('_', '?'), 1), (('?', '_'), 1)])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_finder = BigramCollocationFinder.from_documents([['they', 'can', 'go', 'quite', 'fast'],\n",
    "                                                         ['this', 'was', 'a', 'Royal', 'Enfield', '_', '?', '_']])\n",
    "bigram_freq_dict_aux = bigram_finder.ngram_fd.items()\n",
    "bigram_freq_dict_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build again `bigram_freq_dict_aux` using all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:25.250574Z",
     "start_time": "2022-01-25T10:10:25.200628Z"
    }
   },
   "outputs": [],
   "source": [
    "corrected_sentences = [train[n]['corrected'] for n in range(len(train))]\n",
    "finder = BigramCollocationFinder.from_documents(corrected_sentences)\n",
    "bigram_freq_dict = dict(finder.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:25.661991Z",
     "start_time": "2022-01-25T10:10:25.642600Z"
    }
   },
   "outputs": [],
   "source": [
    "def prob_word(word, word_to_count, n_total_words, n_vocabulary):\n",
    "    # Write your code here.\n",
    "    word = word.lower()\n",
    "    word_counts = word_to_count[word]\n",
    "    return word_counts / n_total_words\n",
    "\n",
    "#def prob_word(word, word_to_count, n_total_words, n_vocabulary): \n",
    "#    return (word_to_count[word]+1) / (n_total_words+ n_vocabulary)\n",
    "\n",
    "def bigrams_starting_by(word, bigram_freq_dictionary): \n",
    "    return [t for t in list(bigram_freq_dict.keys()) if t[0] == word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:26.569552Z",
     "start_time": "2022-01-25T10:10:26.550632Z"
    }
   },
   "outputs": [],
   "source": [
    "n_total_words, len(unique_corrected_words)\n",
    "n_vocabulary = len(unique_corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:26.781588Z",
     "start_time": "2022-01-25T10:10:26.760373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 'see'),\n",
       " ('dog', 'and'),\n",
       " ('dog', 'run'),\n",
       " ('dog', 'is'),\n",
       " ('dog', ','),\n",
       " ('dog', 'had'),\n",
       " ('dog', 'for'),\n",
       " ('dog', 'Toby'),\n",
       " ('dog', '.'),\n",
       " ('dog', 'it')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_starting_by('dog', bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:27.188324Z",
     "start_time": "2022-01-25T10:10:27.169046Z"
    }
   },
   "outputs": [],
   "source": [
    "def return_dictionary_value(bigram, bigram_freq_dict):\n",
    "    try:\n",
    "        return bigram_freq_dict[bigram]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def count_bigrams(list_bigrams, bigram_freq_dict): \n",
    "    return sum([return_dictionary_value(bigram, bigram_freq_dict) for bigram in list_bigrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:27.403648Z",
     "start_time": "2022-01-25T10:10:27.381628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_bigrams([('they','can')], bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:27.706217Z",
     "start_time": "2022-01-25T10:10:27.686817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12218"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:28.787808Z",
     "start_time": "2022-01-25T10:10:28.766394Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def probability_bigram(word1, word2, bigram_freq_dict):\n",
    "    if count_bigrams([(word1,word2)], bigram_freq_dict) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return count_bigrams([(word1,word2)], bigram_freq_dict)/count_bigrams(bigrams_starting_by(word1,bigram_freq_dict), bigram_freq_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:29.330843Z",
     "start_time": "2022-01-25T10:10:29.310302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09090909090909091"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1, word2  = ('dog','is')\n",
    "probability_bigram(word1,word2, bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:30.116934Z",
     "start_time": "2022-01-25T10:10:30.095649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1263537906137184"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1, word2  = ('was','a')\n",
    "probability_bigram(word1,word2, bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:31.441250Z",
     "start_time": "2022-01-25T10:10:31.422154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1, word2  = ('was','cat')\n",
    "probability_bigram(word1, word2, bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:31.868142Z",
     "start_time": "2022-01-25T10:10:31.849001Z"
    }
   },
   "outputs": [],
   "source": [
    "def interpolation_probability(word1, word2, bigram_freq_dict, n_vocabulary, lambda_1 = 0.3): \n",
    "    return (1-lambda_1)*probability_bigram(word1, word2, bigram_freq_dict) +\\\n",
    "            lambda_1*prob_word(word2, unigrams, n_total_words, n_vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:32.747239Z",
     "start_time": "2022-01-25T10:10:32.727214Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09686619623303266"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1, word2  = ('was','a')\n",
    "interpolation_probability(word1, word2, bigram_freq_dict, n_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:32.988474Z",
     "start_time": "2022-01-25T10:10:32.968517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00020633683341739648"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1, word2  = ('was','cat')\n",
    "interpolation_probability(word1, word2, bigram_freq_dict, n_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:33.165027Z",
     "start_time": "2022-01-25T10:10:33.145221Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_candidates(token, vocabulary, max_dist):\n",
    "    distance_token_to_words = {word:edit_distance(word,token.lower()) for word in vocabulary}\n",
    "    minimum_distance = min(distance_token_to_words.values())\n",
    "    if minimum_distance < max_dist:\n",
    "        return sorted([word for word, distance in distance_token_to_words.items() if distance == minimum_distance])\n",
    "    return [token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:33.395681Z",
     "start_time": "2022-01-25T10:10:33.333951Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['house', 'huge', 'use']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidates('huse', vocabulary, max_dist=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:33.606319Z",
     "start_time": "2022-01-25T10:10:33.531691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ditch', 'witch']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidates('kitch', vocabulary, max_dist=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:33.743451Z",
     "start_time": "2022-01-25T10:10:33.723374Z"
    }
   },
   "outputs": [],
   "source": [
    "#%timeit get_candidates('hose', vocabulary, max_dist=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:33.973658Z",
     "start_time": "2022-01-25T10:10:33.952899Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def correct_with_bigrams(sentence, vocabulary):\n",
    "    for index,word in enumerate(sentence):\n",
    "        if ((word and word.lower()) not in vocabulary) and (not word[0].isupper()):\n",
    "            if index == 0: \n",
    "                previous_word = '.'\n",
    "            else:\n",
    "                previous_word = sentence[index-1].lower()\n",
    "            candidates = {candidate:interpolation_probability(previous_word, candidate, bigram_freq_dict,\n",
    "                                                              n_vocabulary, lambda_1=0.3) for candidate in get_candidates(word,vocabulary,max_dist=2)}\n",
    "            \n",
    "            \n",
    "            sentence[index] = max(candidates, key=candidates.get)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:34.214181Z",
     "start_time": "2022-01-25T10:10:34.149526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'big', 'house']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_with_bigrams([\"the\",\"big\",\"hose\"], vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:34.380647Z",
     "start_time": "2022-01-25T10:10:34.310217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hole': 0.0019202404016783775,\n",
       " 'home': 0.0006602778669356688,\n",
       " 'hope': 0.00012380210005043788,\n",
       " 'horse': 1.37557888944931e-05,\n",
       " 'hoses': 1.37557888944931e-05,\n",
       " 'house': 0.00660255290938049,\n",
       " 'lose': 4.12673666834793e-05,\n",
       " 'nose': 4.12673666834793e-05,\n",
       " 'rose': 6.87789444724655e-05,\n",
       " 'those': 2.75115777889862e-05,\n",
       " 'whose': 1.37557888944931e-05}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'hose'\n",
    "previous_word = 'the'\n",
    "candidates = {candidate:interpolation_probability(previous_word, candidate, bigram_freq_dict,n_vocabulary, lambda_1=0.3) for candidate in get_candidates(word,vocabulary,max_dist=2)}\n",
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:41.646110Z",
     "start_time": "2022-01-25T10:10:35.202063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9014084507042254"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy_bigrams(test, vocabulary):\n",
    "    # Write your code here\n",
    "    count_total_words = 0\n",
    "    count_corrected_words = 0\n",
    "    mistakes = []\n",
    "    for m,sentence in enumerate(test):\n",
    "        s_true = sentence['corrected']\n",
    "        s_hat  = correct_with_bigrams(sentence['original'].copy(), vocabulary)\n",
    "        count_total_words  += len(s_true)\n",
    "        correct_predictions = sum(s_hat[n] == s_true[n] for n in range(len(s_true)))\n",
    "        count_corrected_words += correct_predictions\n",
    "        if correct_predictions != len(s_true):\n",
    "            mistakes.append(m)\n",
    "            \n",
    "    return count_corrected_words/count_total_words, mistakes\n",
    "\n",
    "acc, mistakes = accuracy_bigrams(test, vocabulary)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:42.055038Z",
     "start_time": "2022-01-25T10:10:41.832977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistake indices = [8, 12]\n",
      "x      = ['1', 'night', 'when', 'it', 'was', 'dark', 'about', '12', 'oclock', 'a', 'man', 'was', 'nock', 'down', 'by', 'a', 'car', '.']\n",
      "y_hat  = ['1', 'night', 'when', 'it', 'was', 'dark', 'about', '12', \"o'clock\", 'a', 'man', 'was', 'knock', 'down', 'by', 'a', 'car', '.']\n",
      "y_true = ['1', 'night', 'when', 'it', 'was', 'dark', 'about', '12', \"o'clock\", 'a', 'man', 'was', 'knocked', 'down', 'by', 'a', 'car', '.']\n"
     ]
    }
   ],
   "source": [
    "i = 14\n",
    "sentence = test[mistakes[i]]\n",
    "x = sentence[\"original\"]\n",
    "y_true = sentence[\"corrected\"]\n",
    "y_hat  = correct_with_bigrams(sentence['original'].copy(), vocabulary)\n",
    "\n",
    "print(f'mistake indices = {sentence[\"indexes\"]}')\n",
    "print(f'x      = {x}')\n",
    "print(f'y_hat  = {y_hat}')\n",
    "print(f'y_true = {y_true}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:42.343048Z",
     "start_time": "2022-01-25T10:10:42.161718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistakes[i]=9\n",
      "mistake indices = [9]\n",
      "x      = ['I', 'go', 'to', 'bed', 'at', '10', 'o', 'clock', 'I', 'wakh', 'TV', 'at', '5', 'o', 'clock', 'I', 'live', 'in', 'a', 'house', '.']\n",
      "y_hat  = ['I', 'go', 'to', 'bed', 'at', '10', 'o', 'block', 'I', 'wash', 'TV', 'at', '5', 'o', 'block', 'I', 'live', 'in', 'a', 'house', '.']\n",
      "y_true = ['I', 'go', 'to', 'bed', 'at', '10', 'o', 'clock', 'I', 'watch', 'TV', 'at', '5', 'o', 'clock', 'I', 'live', 'in', 'a', 'house', '.']\n"
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "sentence = test[mistakes[i]]\n",
    "\n",
    "x      = sentence[\"original\"]\n",
    "y_true = sentence[\"corrected\"]\n",
    "y_hat  = correct_with_bigrams(sentence['original'].copy(), vocabulary)\n",
    "\n",
    "print(f'mistakes[i]={mistakes[i]}')\n",
    "print(f'mistake indices = {sentence[\"indexes\"]}')\n",
    "print(f'x      = {x}')\n",
    "print(f'y_hat  = {y_hat}')\n",
    "print(f'y_true = {y_true}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:42.459866Z",
     "start_time": "2022-01-25T10:10:42.438430Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': ['on', 'sundays', 'I', 'go', 'to', 'church', '.'],\n",
       " 'corrected': ['on', 'sundays', 'I', 'go', 'to', 'church', '.'],\n",
       " 'indexes': []}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T10:10:42.682900Z",
     "start_time": "2022-01-25T10:10:42.550072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistakes[i]=33\n",
      "mistake indices = [15, 26]\n",
      "x      = ['The', 'murder', 'man', 'has', 'a', 'black', 'beard', 'The', 'next', 'day', 'one', 'of', 'the', 'policemen', 'were', 'killd', 'the', 'next', 'day', 'they', 'found', 'the', 'car', 'over', 'the', 'Hill', 'the', 'was', 'the', 'man', 'near', 'it', 'he', 'was', 'dead', '.']\n",
      "y_hat  = ['The', 'murder', 'man', 'has', 'a', 'black', 'heard', 'The', 'next', 'day', 'one', 'of', 'the', 'policemen', 'were', 'killed', 'the', 'next', 'day', 'they', 'found', 'the', 'car', 'over', 'the', 'Hill', 'the', 'was', 'the', 'man', 'near', 'it', 'he', 'was', 'dead', '.']\n",
      "y_true = ['The', 'murder', 'man', 'has', 'a', 'black', 'beard', 'The', 'next', 'day', 'one', 'of', 'the', 'policemen', 'were', 'killed', 'the', 'next', 'day', 'they', 'found', 'the', 'car', 'over', 'the', 'Hill', 'there', 'was', 'the', 'man', 'near', 'it', 'he', 'was', 'dead', '.']\n"
     ]
    }
   ],
   "source": [
    "i = 20\n",
    "sentence = test[mistakes[i]]\n",
    "x      = sentence[\"original\"]\n",
    "y_true = sentence[\"corrected\"]\n",
    "y_hat  = correct_with_bigrams(sentence['original'].copy(), vocabulary)\n",
    "\n",
    "\n",
    "print(f'mistakes[i]={mistakes[i]}')\n",
    "print(f'mistake indices = {sentence[\"indexes\"]}')\n",
    "print(f'x      = {x}')\n",
    "print(f'y_hat  = {y_hat}')\n",
    "print(f'y_true = {y_true}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
