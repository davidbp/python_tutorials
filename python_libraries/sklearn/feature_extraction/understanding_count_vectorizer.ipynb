{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "# Count Vectorizer basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:37:49.255297Z",
     "start_time": "2022-02-23T16:37:43.684202Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.sparse as sp\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.datasets\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:37:49.261461Z",
     "start_time": "2022-02-23T16:37:49.257375Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.__version__=1.0.2\n"
     ]
    }
   ],
   "source": [
    "print(f'sklearn.__version__={sklearn.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ['this is a set', 'of data to train a feature vector vectorizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:38.578688Z",
     "start_time": "2021-03-01T12:00:36.014092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_count_vectorizer = CountVectorizer()\n",
    "simple_count_vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:38.585652Z",
     "start_time": "2021-03-01T12:00:38.580902Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [\"this is a feature vector for this sentence\"]\n",
    "x = simple_count_vectorizer.transform(A)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an empty sentence will produce a vector or zeros\n",
    "A = [\"this is a feature vector for this sentence\", \"\"]\n",
    "x = simple_count_vectorizer.transform(A)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:38.594035Z",
     "start_time": "2021-03-01T12:00:38.587408Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [\"this is a feature vector, for this sentence\", \"This is another sentence\"]\n",
    "x = simple_count_vectorizer.transform(A)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:38.599221Z",
     "start_time": "2021-03-01T12:00:38.595757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input string: this is a feature vector, for this sentence\n",
      "tokenized string: ['this', 'is', 'feature', 'vector', 'for', 'this', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "tokenizer_func = simple_count_vectorizer.build_tokenizer()\n",
    "print('input string:', A[0])\n",
    "print('tokenized string:', tokenizer_func(A[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:38.604218Z",
     "start_time": "2021-03-01T12:00:38.600690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'feature', 'vector', 'for', 'this', 'sentence']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_vocab = [w for w in tokenizer_func(A[0]) if w in simple_count_vectorizer.vocabulary_]\n",
    "words_in_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:38.608760Z",
     "start_time": "2021-03-01T12:00:38.605703Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 6)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_in_vocab), len(set(words_in_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:38.613833Z",
     "start_time": "2021-03-01T12:00:38.610189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_notin_vocab = [w for w in tokenizer_func(A[0]) if w not in simple_count_vectorizer.vocabulary_]\n",
    "words_notin_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:42:17.849502Z",
     "start_time": "2022-02-23T16:42:16.560143Z"
    }
   },
   "outputs": [],
   "source": [
    "X = sklearn.datasets.fetch_20newsgroups()\n",
    "\n",
    "X_train = sklearn.datasets.fetch_20newsgroups(subset=\"train\").data\n",
    "y_train = sklearn.datasets.fetch_20newsgroups(subset=\"train\").target\n",
    "X_test  = sklearn.datasets.fetch_20newsgroups(subset=\"test\").data\n",
    "y_test  = sklearn.datasets.fetch_20newsgroups(subset=\"test\").target\n",
    "\n",
    "#y_train = y_train.reshape(-1,1)\n",
    "#y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:42:58.114348Z",
     "start_time": "2022-02-23T16:42:58.109951Z"
    }
   },
   "outputs": [],
   "source": [
    "x = X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:43:06.035635Z",
     "start_time": "2022-02-23T16:43:06.028683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:43:42.526207Z",
     "start_time": "2022-02-23T16:43:42.517808Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1))\n",
    "count_vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that many ngrams from the vocabulary might be quite meaningless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:08:50.387678Z",
     "start_time": "2021-03-01T12:08:50.360131Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000',\n",
       " '00000',\n",
       " '000000',\n",
       " '00000000',\n",
       " '0000000004',\n",
       " '0000000005',\n",
       " '00000000b',\n",
       " '00000001',\n",
       " '00000001b',\n",
       " '0000000667',\n",
       " '00000010',\n",
       " '00000010b',\n",
       " '00000011',\n",
       " '00000011b',\n",
       " '0000001200',\n",
       " '00000074',\n",
       " '00000093',\n",
       " '000000e5']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = list(count_vectorizer.vocabulary_)\n",
    "vocabulary.sort()\n",
    "vocabulary[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearn the vocabulary removing features that are too rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130107"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `min_df`: control  minimum frequency to add a word in the vocabulary\n",
    "\n",
    "We can control the minimum number of times a word has to be seen in order to include it in the vocabulary with `min_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(min_df=10)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer_min_df = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1), min_df= 10)\n",
    "count_vectorizer_min_df.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15593"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vectorizer_min_df.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0002',\n",
       " '001',\n",
       " '0062',\n",
       " '01',\n",
       " '0111',\n",
       " '02',\n",
       " '0200',\n",
       " '02115',\n",
       " '02139',\n",
       " '02238',\n",
       " '03',\n",
       " '030',\n",
       " '0358',\n",
       " '0366',\n",
       " '04',\n",
       " '040',\n",
       " '0400',\n",
       " '05']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = list(count_vectorizer_min_df.vocabulary_)\n",
    "vocabulary.sort()\n",
    "vocabulary[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `max_features`: control max amount of features in the vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=10000)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer_max_features = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1), max_features= 10000)\n",
    "count_vectorizer_max_features.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vectorizer_max_features.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '005',\n",
       " '01',\n",
       " '02',\n",
       " '02238',\n",
       " '02p',\n",
       " '03',\n",
       " '030',\n",
       " '0358',\n",
       " '04',\n",
       " '040',\n",
       " '0400',\n",
       " '05',\n",
       " '06',\n",
       " '07',\n",
       " '08',\n",
       " '09',\n",
       " '0b',\n",
       " '0c']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = list(count_vectorizer_max_features.vocabulary_)\n",
    "vocabulary.sort()\n",
    "vocabulary[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer Options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"I like chocolate, a lot!\",\n",
    "          \"I like 1080p panels\",\n",
    "          \"I love build-in speakers\",\n",
    "          \"Z-Edge UG24 24-inch Curved Gaming Monitor 180Hz Refresh Rate, 1ms MPRT, FHD 1080p Gaming Monitor, R1650 Curved\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The default `CountVectorizer` might not be doing exactly what we want.\n",
    "\n",
    "A default `CountVectorizer` using the previous `corpus` will show the following properties\n",
    "\n",
    "- (I) Words joined by '-' such as \"build-in\", \"24-inch\" or \"Z-edge\" are splitted as two tokens:\n",
    "- (II) Single character words are removed such as `I`\n",
    "    \n",
    "\n",
    "### Customising Vectoriser classes\n",
    "\n",
    "- **preprocessor**: a callable that takes an entire document as input (as a single string), and returns a possibly transformed version of the document, still as an entire string. This can be used to remove HTML tags, lowercase the entire document, etc.\n",
    "\n",
    "\n",
    "- **tokenizer**: a callable that takes the output from the preprocessor and splits it into tokens, then returns a list of these.\n",
    "\n",
    "\n",
    "- **analyzer**: a callable that replaces the preprocessor and tokenizer. The default analyzers all call the preprocessor and tokenizer, but custom analyzers will skip this. N-gram extraction and stop word filtering take place at the analyzer level, so a custom analyzer may have to reproduce these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `token_pattern`: control the regex to create tokens\n",
    "\n",
    "We can control how tokens are generated with `token_pattern`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'like': 12,\n",
       " 'chocolate': 5,\n",
       " 'lot': 13,\n",
       " '1080p': 0,\n",
       " 'panels': 17,\n",
       " 'love': 14,\n",
       " 'build': 4,\n",
       " 'in': 10,\n",
       " 'speakers': 21,\n",
       " 'edge': 7,\n",
       " 'ug24': 22,\n",
       " '24': 3,\n",
       " 'inch': 11,\n",
       " 'curved': 6,\n",
       " 'gaming': 9,\n",
       " 'monitor': 15,\n",
       " '180hz': 1,\n",
       " 'refresh': 20,\n",
       " 'rate': 19,\n",
       " '1ms': 2,\n",
       " 'mprt': 16,\n",
       " 'fhd': 8,\n",
       " 'r1650': 18}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(corpus)\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 10,\n",
       " 'like': 11,\n",
       " 'chocolate': 6,\n",
       " 'a': 4,\n",
       " 'lot': 12,\n",
       " '1080p': 0,\n",
       " 'panels': 16,\n",
       " 'love': 13,\n",
       " 'build-in': 5,\n",
       " 'speakers': 20,\n",
       " 'z-edge': 22,\n",
       " 'ug24': 21,\n",
       " '24-inch': 3,\n",
       " 'curved': 7,\n",
       " 'gaming': 9,\n",
       " 'monitor': 14,\n",
       " '180hz': 1,\n",
       " 'refresh': 19,\n",
       " 'rate': 18,\n",
       " '1ms': 2,\n",
       " 'mprt': 15,\n",
       " 'fhd': 8,\n",
       " 'r1650': 17}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_pattern = r\"\\w+[\\-\\'\\%\\+\\.\\\"]?\\w*[\\-\\'\\%\\+\\.\\\"]?\\w*\"\n",
    "count_vectorizer = CountVectorizer(token_pattern=token_pattern)\n",
    "count_vectorizer.fit(corpus)\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `tokenizer`: control the tokenization process directly\n",
    "\n",
    "We can control how tokens are generated pasing a custom `tokenizer` function.\n",
    "\n",
    "- **Warning**: This tokenizer is really simple and generates tokens such as `lot!`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    return s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 10,\n",
       " 'like': 11,\n",
       " 'chocolate,': 6,\n",
       " 'a': 4,\n",
       " 'lot!': 12,\n",
       " '1080p': 0,\n",
       " 'panels': 17,\n",
       " 'love': 13,\n",
       " 'build-in': 5,\n",
       " 'speakers': 21,\n",
       " 'z-edge': 23,\n",
       " 'ug24': 22,\n",
       " '24-inch': 3,\n",
       " 'curved': 7,\n",
       " 'gaming': 9,\n",
       " 'monitor': 14,\n",
       " '180hz': 1,\n",
       " 'refresh': 20,\n",
       " 'rate,': 19,\n",
       " '1ms': 2,\n",
       " 'mprt,': 16,\n",
       " 'fhd': 8,\n",
       " 'monitor,': 15,\n",
       " 'r1650': 18}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(tokenizer=my_tokenizer)\n",
    "count_vectorizer.fit(corpus)\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `analyzer`: control how the ngrams are generated from an input string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'there', 'want']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "tokenizer = count_vectorizer.build_tokenizer()\n",
    "tokenizer('hello there I want')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bosh',\n",
       " 'drill',\n",
       " 'high',\n",
       " 'quality',\n",
       " 'low',\n",
       " 'vibration',\n",
       " 'bosh',\n",
       " 'trust',\n",
       " 'bosh drill',\n",
       " 'drill high',\n",
       " 'high quality',\n",
       " 'quality low',\n",
       " 'low vibration',\n",
       " 'vibration bosh',\n",
       " 'bosh trust']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'bosh drill, high quality, low vibration, bosh trust'\n",
    "token_pattern = r\"\\w+[\\-\\'\\%\\+\\.\\\"]?\\w*[\\-\\'\\%\\+\\.\\\"]?\\w*\"\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,2), token_pattern = token_pattern)\n",
    "analyzer = count_vectorizer.build_analyzer()\n",
    "analyzer(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bosh', 'drill', 'bosh drill'],\n",
       " ['high', 'quality', 'high quality'],\n",
       " ['low', 'vibration', 'low vibration'],\n",
       " ['bosh', 'trust', 'bosh trust']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_analyzer(s, analyzer_from_count_vectorizer):\n",
    "    \"\"\"\n",
    "    This analyzer can contain the same feature several times\n",
    "    \"\"\"\n",
    "    subsentences = re.split(r'[\\.\\|\\,\\:\\，]\\D', s)\n",
    "    result = []\n",
    "    for subsentence in subsentences:\n",
    "        result.append(analyzer_from_count_vectorizer(subsentence))\n",
    "    return result\n",
    "\n",
    "custom_analyzer(s, analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.6 µs ± 43.5 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  custom_analyzer(s, analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'drill',\n",
       " 'great drill',\n",
       " 'bosh',\n",
       " 'drill',\n",
       " 'bosh drill',\n",
       " 'high',\n",
       " 'quality',\n",
       " 'high quality',\n",
       " 'low',\n",
       " 'vibration',\n",
       " 'low vibration',\n",
       " 'bosh',\n",
       " 'trust',\n",
       " 'bosh trust']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '[great drill]: bosh drill, high quality, low vibration, bosh trust'\n",
    "\n",
    "token_pattern = r\"\\w+[\\-\\'\\%\\+\\.\\\"]?\\w*[\\-\\'\\%\\+\\.\\\"]?\\w*\"\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,2), token_pattern = token_pattern)\n",
    "token_pattern_subsentences = r'[\\，\\,\\|\\:\\/,\\.\\-\\：\\–]\\s'\n",
    "token_pattern_subsentences_compiled = re.compile(token_pattern_subsentences)\n",
    "\n",
    "def custom_analyzer(s, analyzer_from_count_vectorizer):\n",
    "    subsentences = token_pattern_subsentences_compiled.split(s)\n",
    "    result = []\n",
    "    for subsentence in subsentences:\n",
    "        for t in analyzer_from_count_vectorizer(subsentence):\n",
    "            result.append(t)\n",
    "    return result\n",
    "\n",
    "custom_analyzer(s, analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.7 µs ± 53.3 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit custom_analyzer(s, analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
