{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#model-selection\" data-toc-modified-id=\"model-selection-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>model selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Finding-the-indices-for-KFold\" data-toc-modified-id=\"Finding-the-indices-for-KFold-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Finding the indices for <code>KFold</code></a></span></li><li><span><a href=\"#LeaveOneGroupOut\" data-toc-modified-id=\"LeaveOneGroupOut-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span><code>LeaveOneGroupOut</code></a></span></li></ul></li><li><span><a href=\"#Training-models-in-crosvalidation-using-GridsearchCV\" data-toc-modified-id=\"Training-models-in-crosvalidation-using-GridsearchCV-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Training models in crosvalidation using GridsearchCV</a></span><ul class=\"toc-item\"><li><span><a href=\"#Saved-metrics-in-a-GridSearchCV-object\" data-toc-modified-id=\"Saved-metrics-in-a-GridSearchCV-object-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Saved metrics in a <code>GridSearchCV</code> object</a></span><ul class=\"toc-item\"><li><span><a href=\"#Configurations-of--Hyperparameters-tested\" data-toc-modified-id=\"Configurations-of--Hyperparameters-tested-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Configurations of  Hyperparameters tested</a></span></li></ul></li></ul></li><li><span><a href=\"#Generating-all-possible-combinations-of-hyperparameters\" data-toc-modified-id=\"Generating-all-possible-combinations-of-hyperparameters-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Generating all possible combinations of hyperparameters</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ranking-results-of-the-crossvalidation-process\" data-toc-modified-id=\"Ranking-results-of-the-crossvalidation-process-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Ranking results of the crossvalidation process</a></span></li><li><span><a href=\"#Creating-your-own-GridSearchCV\" data-toc-modified-id=\"Creating-your-own-GridSearchCV-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Creating your own <code>GridSearchCV</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Training-using-crosvalidation-a-given-model\" data-toc-modified-id=\"Training-using-crosvalidation-a-given-model-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Training using crosvalidation a given model</a></span></li><li><span><a href=\"#TODO:-Make-a-function-that-given-cv_results_df-and-path_folder...\" data-toc-modified-id=\"TODO:-Make-a-function-that-given-cv_results_df-and-path_folder...-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>TODO: Make a function that given cv_results_df and path_folder...</a></span></li></ul></li></ul></li><li><span><a href=\"#Finding-indices-with-GroupKFold\" data-toc-modified-id=\"Finding-indices-with-GroupKFold-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Finding indices with <code>GroupKFold</code></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Finding the indices for `KFold`\n",
    "\n",
    "The class `KFold` allows us to generate the train and validation indicies for performing training and validation indicies with different parts of our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "X = np.random.randn(100,4)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0 1 2 3 4 5 6 7 8 9]\n",
      "1 [10 11 12 13 14 15 16 17 18 19]\n",
      "2 [20 21 22 23 24 25 26 27 28 29]\n",
      "3 [30 31 32 33 34 35 36 37 38 39]\n",
      "4 [40 41 42 43 44 45 46 47 48 49]\n",
      "5 [50 51 52 53 54 55 56 57 58 59]\n",
      "6 [60 61 62 63 64 65 66 67 68 69]\n",
      "7 [70 71 72 73 74 75 76 77 78 79]\n",
      "8 [80 81 82 83 84 85 86 87 88 89]\n",
      "9 [90 91 92 93 94 95 96 97 98 99]\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(10,shuffle=False)\n",
    "splits = folds.split(X)\n",
    "for fold, (tr_ind,va_ind) in enumerate(splits):\n",
    "    print(fold, va_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If shuffle is true then we sample all rows from our dataset randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9 19 29 46 68 84 85 86 89 98]\n",
      "[ 6 15 35 38 53 60 77 92 93 94]\n",
      "[21 23 28 49 58 62 72 76 88 96]\n",
      "[ 3  4 13 34 43 52 54 57 80 81]\n",
      "[ 0 10 24 27 32 42 47 73 97 99]\n",
      "[17 22 25 30 40 75 79 82 90 95]\n",
      "[12 18 33 37 50 64 65 78 83 91]\n",
      "[11 16 20 36 41 44 51 55 66 71]\n",
      "[ 1  5 26 39 48 59 61 70 74 87]\n",
      "[ 2  7  8 14 31 45 56 63 67 69]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folds = KFold(10,shuffle=True)\n",
    "splits = folds.split(X)\n",
    "for tr_ind,va_ind in splits:\n",
    "    print(va_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  `LeaveOneGroupOut`\n",
    "\n",
    "- Provides train/test indices to split data according to a third-party provided group. \n",
    "\n",
    "- This group information can be used to encode arbitrary domain specific stratifications of the samples as integers.\n",
    "\n",
    "\n",
    "Now let us load the boston dataset and make a group that we will call `not_24`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "dataset = sklearn.datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.data\n",
    "y = dataset.target\n",
    "X_df = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4      5     6       7    8      9     10  \\\n",
       "0  0.00632  18.0  2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0  15.3   \n",
       "1  0.02731   0.0  7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0  17.8   \n",
       "2  0.02729   0.0  7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0  17.8   \n",
       "3  0.03237   0.0  2.18  0.0  0.458  6.998  45.8  6.0622  3.0  222.0  18.7   \n",
       "4  0.06905   0.0  2.18  0.0  0.458  7.147  54.2  6.0622  3.0  222.0  18.7   \n",
       "\n",
       "       11    12  \n",
       "0  396.90  4.98  \n",
       "1  396.90  9.14  \n",
       "2  392.83  4.03  \n",
       "3  394.63  2.94  \n",
       "4  396.90  5.33  "
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3.,  5.,  4.,  8.,  6.,  7., 24.])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df[8].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider a case where we might want the train splits to be divided in two groups A and B\n",
    "\n",
    "- A: examples which at column 8 have values in `[1,  2,  3,  4,  5,  6,  7,  8]` \n",
    "- B: examples at which column 9 takes value  `24`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_24 = X_df[8] != 24\n",
    "value_24 = np.array(value_24.values, dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506,))"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, not_24.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=LeaveOneGroupOut(), error_score='raise-deprecating',\n",
       "       estimator=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1, param_grid={},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import neural_network\n",
    "\n",
    "cv   = sklearn.model_selection.LeaveOneGroupOut()\n",
    "clf  = sklearn.linear_model.LinearRegression()\n",
    "grid = sklearn.model_selection.GridSearchCV(clf, param_grid={}, cv=cv, \n",
    "                                            return_train_score=True, iid=True, n_jobs=-1)\n",
    "\n",
    "grid.fit(X,y, groups=value_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00095105]),\n",
       " 'std_fit_time': array([6.19888306e-05]),\n",
       " 'mean_score_time': array([0.00039613]),\n",
       " 'std_score_time': array([1.07288361e-06]),\n",
       " 'params': [{}],\n",
       " 'split0_test_score': array([-6.59688586]),\n",
       " 'split1_test_score': array([-6.75857278e+19]),\n",
       " 'mean_test_score': array([-4.99546684e+19]),\n",
       " 'std_test_score': array([2.96774953e+19]),\n",
       " 'rank_test_score': array([1], dtype=int32),\n",
       " 'split0_train_score': array([0.86686183]),\n",
       " 'split1_train_score': array([0.68323554]),\n",
       " 'mean_train_score': array([0.77504868]),\n",
       " 'std_train_score': array([0.09181314])}"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.86686183]), array([-6.75857278e+19]))"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_['split0_train_score'], grid.cv_results_['split1_test_score'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that one of the errors (the one in split1) is huge. \n",
    "\n",
    "This happens because one of the experiments in the GridSearchCV will consist on taking the examples  from B as training set (and it is very small) and then use the values in A as validation set (which is very big)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=LeaveOneGroupOut(), error_score='raise-deprecating',\n",
       "       estimator=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1, param_grid={},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import neural_network\n",
    "\n",
    "cv   = sklearn.model_selection.LeaveOneGroupOut()\n",
    "clf  = sklearn.linear_model.LinearRegression()\n",
    "grid = sklearn.model_selection.GridSearchCV(clf, param_grid={}, cv=cv, \n",
    "                                            return_train_score=True, iid=True, n_jobs=-1)\n",
    "\n",
    "values_in_col8 = np.array(X_df[8].values, dtype=\"int\")\n",
    "\n",
    "grid.fit(X,y, groups=values_in_col8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00112086]),\n",
       " 'std_fit_time': array([0.000122]),\n",
       " 'mean_score_time': array([0.00059387]),\n",
       " 'std_score_time': array([3.01011142e-05]),\n",
       " 'params': [{}],\n",
       " 'split0_test_score': array([0.70028178]),\n",
       " 'split1_test_score': array([0.77076574]),\n",
       " 'split2_test_score': array([0.64766753]),\n",
       " 'split3_test_score': array([0.70385404]),\n",
       " 'split4_test_score': array([0.64780526]),\n",
       " 'split5_test_score': array([-1.95031015]),\n",
       " 'split6_test_score': array([0.3699049]),\n",
       " 'split7_test_score': array([0.69561196]),\n",
       " 'split8_test_score': array([-6.59688586]),\n",
       " 'mean_test_score': array([-1.36260276]),\n",
       " 'std_test_score': array([3.16246777]),\n",
       " 'rank_test_score': array([1], dtype=int32),\n",
       " 'split0_train_score': array([0.74100996]),\n",
       " 'split1_train_score': array([0.73639286]),\n",
       " 'split2_train_score': array([0.7369053]),\n",
       " 'split3_train_score': array([0.73722303]),\n",
       " 'split4_train_score': array([0.71793616]),\n",
       " 'split5_train_score': array([0.74817225]),\n",
       " 'split6_train_score': array([0.74326855]),\n",
       " 'split7_train_score': array([0.73115118]),\n",
       " 'split8_train_score': array([0.86686183]),\n",
       " 'mean_train_score': array([0.75099123]),\n",
       " 'std_train_score': array([0.04174068])}"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models in crosvalidation using GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sklearn.datasets.california_housing.fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20640, 8), (20640,))"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15480, 8), (15480,), (5160, 8), (5160,))"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr, X_te, y_tr, y_te = sklearn.model_selection.train_test_split(X,y, random_state=1234)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import *\n",
    "\n",
    "rf = sklearn.ensemble.RandomForestRegressor()\n",
    "\n",
    "param_grid = {\"max_depth\":[5,None,10], \"max_features\":[\"auto\",0.5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid = sklearn.model_selection.GridSearchCV(rf, \n",
    "                                               param_grid=param_grid,\n",
    "                                               cv=4,\n",
    "                                               n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'max_depth': [5, None, 10], 'max_features': ['auto', 0.5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=0.5, max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saved metrics in a `GridSearchCV` object\n",
    "\n",
    "After fitting a `GridsearchCV` object we can inspect the information saved during crossvalidation inside the field `.cv_results_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean_fit_time',\n",
       " 'std_fit_time',\n",
       " 'mean_score_time',\n",
       " 'std_score_time',\n",
       " 'param_max_depth',\n",
       " 'param_max_features',\n",
       " 'params',\n",
       " 'split0_test_score',\n",
       " 'split1_test_score',\n",
       " 'split2_test_score',\n",
       " 'split3_test_score',\n",
       " 'mean_test_score',\n",
       " 'std_test_score',\n",
       " 'rank_test_score',\n",
       " 'split0_train_score',\n",
       " 'split1_train_score',\n",
       " 'split2_train_score',\n",
       " 'split3_train_score',\n",
       " 'mean_train_score',\n",
       " 'std_train_score']"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(rf_grid.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Configurations of  Hyperparameters tested\n",
    "\n",
    "All the combinations of parameters tested are kept in `'params'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.cv_results_[\"params\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different scores for each of the CV splits are found in `splitsK_test_score`.\n",
    "\n",
    "In this case we have 4 arrays because we used `cv=4` when we instanciated `rf_grid`.\n",
    "\n",
    "```\n",
    "'split0_test_score'\n",
    "'split1_test_score'\n",
    "'split2_test_score'\n",
    "'split3_test_score'\n",
    " ```\n",
    " \n",
    " Each array contains  possition `k` the test results of the k'th combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.n_splits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = [rf_grid.cv_results_[f\"split{i}_test_score\" ] for i in range(4)]\n",
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `rf_grid.best_score_` is different than `np.max(all_scores)`.\n",
    "\n",
    "The best score is found using the mean value of the crossvalidation scores.\n",
    "\n",
    "Therefore `rf_grid.best_score_= np.max(rf_grid.cv_results_[\"mean_test_score\"])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(rf_grid.cv_results_[\"mean_test_score\"]), rf_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.cv_results_[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we can compute `mean_test_score` simpy computing the mean\n",
    "over the different results in the different splits of the crossvalidation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_scores).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating all possible combinations of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function product can take as input several iterators and it will generate\n",
    "all the combinations of the values in the iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in product([\"a\",\"b\",\"c\"],[1,2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use it to generate the combinations of different hyperparamaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[x for x in product(param_grid[\"max_depth\"], param_grid[\"max_features\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `*` notation allows us to generate a correctly formated input for `product`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_params_values = [x for x in product(*param_grid.values())]\n",
    "combination_params_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we can then write the name of the param for each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_keys = list(param_grid.keys())\n",
    "\n",
    "for values in combination_params_values:\n",
    "    print(dict(zip(param_grid.keys(),values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do all this in a single function that will generate the list with the combinations we want to explore given an space of hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_params(param_grid):\n",
    "    combination_params_values = [x for x in product(*param_grid.values())]\n",
    "    params = []\n",
    "    for values in combination_params_values:\n",
    "        params.append(dict(zip(param_grid.keys(),values)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_params(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is the same as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_grid.cv_results_[\"params\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking results of the crossvalidation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All combinations of hyperparameters are stored in **`.cv_results_[\"params\"]`**\n",
    "\n",
    "We can visuallize in a single dataframe the differnet pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"params\": rf_grid.cv_results_[\"params\"], \n",
    "                        \"std_test_score\": rf_grid.cv_results_[\"std_test_score\"],\n",
    "                        \"mean_test_score\": rf_grid.cv_results_[\"mean_test_score\"],\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.sort_values(by=[\"mean_test_score\"], ascending=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the order (rank) of each parameter configuration in **`.cv_results_[\"rank_test_score\"]`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_test_score = rf_grid.cv_results_[\"rank_test_score\"]\n",
    "rank_test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that **results of the grid search are sorted by `rf_grid.cv_results_[\"rank_test_score\"]`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[rf_grid.cv_results_[\"params\"][k-1] for k in rank_test_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ranking the results of crossvalidation: 1:best result, 2:second best result etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find in `rank_test_score` a ranking of the best (1) to the worst (6) results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.cv_results_[\"rank_test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate this vector using `scipy.stats.rankdata` in order to rank the solutions according to `mean_test_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "rank = scipy.stats.rankdata(-rf_grid.cv_results_[\"mean_test_score\"],method='ordinal')\n",
    "rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating your own `GridSearchCV` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"max_depth\":[5,None,10], \"max_features\":[\"auto\",0.5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_params(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = sklearn.ensemble.RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using crosvalidation a given model\n",
    "\n",
    "We want to have code that can train for each combination in `param_grid` we would like to train `cv` models and save the scores\n",
    "of the different fitted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_str(model, cv, params_combination):\n",
    "\n",
    "    final_name = type(model).__name__ + \"__fold{:02d}\".format(cv)\n",
    "    for item in params_combination.items():\n",
    "        final_name += \"__\"\n",
    "        final_name += item[0]+\"=\" +str(item[1])\n",
    "        \n",
    "    return final_name + \".joblib\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_combinations = generate_params(param_grid)\n",
    "model_str(rf,2, params_combinations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "\n",
    "def CVfit(X, y, model, cv, param_grid, scorer, path_folder=\"\",\n",
    "          save_models=False, save_summary=True, add_time=False, verbose=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Important notes:\n",
    "    \n",
    "    - By default this method uses the model.score function to score the test results.\n",
    "    \n",
    "    - If `scorer=\"roc_auc_score\"` or `scorer=sklearn.metrics.roc_auc_score` then the method uses \n",
    "    `model.predict_proba` instead of `model.predict` when scoring the predictions.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    cv_results_params = generate_params(param_grid)\n",
    "    folds = KFold(cv,shuffle=False)\n",
    "    cv_results_ = {}\n",
    "    cv_results_[\"params\"] = cv_results_params\n",
    "    \n",
    "    if add_time:\n",
    "        now = datetime.datetime.now()\n",
    "        time_id = f\"__{now.hour}:{now.minute}:{now.second}__{now.day}-{now.month}-{now.year}\"\n",
    "        path_folder = path_folder +  time_id       \n",
    "    \n",
    "    for fold in range(cv):\n",
    "        cv_results_[f\"split{fold}_test_score\"] = np.array([])\n",
    "    \n",
    "    # Create target Directory if don't exist\n",
    "    if not os.path.exists(path_folder):\n",
    "        os.mkdir(path_folder)\n",
    "        print(\"Directory \" , path_folder ,  \" Created \")\n",
    "    else:    \n",
    "        print(\"WARNING: Directory \" , path_folder ,  \" already exists\")\n",
    "\n",
    "    \n",
    "    cvfit_log_file = open(os.path.join(path_folder, 'cvfit_log.txt'), \"w\")\n",
    "\n",
    "        \n",
    "    for param_combination in cv_results_params:\n",
    "        splits = folds.split(X)\n",
    "        model_current = model.__class__(**param_combination)\n",
    "        \n",
    "        for fold, (tr_ind, va_ind) in enumerate(splits):\n",
    "            fold_str = f\"split{fold}_test_score\"\n",
    "            name = model_str(model_current, fold, param_combination)\n",
    "                \n",
    "            model_current.fit(X[tr_ind], y[tr_ind])\n",
    "            \n",
    "            if scorer:\n",
    "                if scorer.__name__ ==  \"roc_auc_score\":\n",
    "                    test_score_fold = scorer(model_current.predict_proba(X[va_ind]), y[va_ind])\n",
    "                elif scorer == \"roc_auc_score\":\n",
    "                    test_score_fold = roc_auc_score(model_current.predict_proba(X[va_ind]), y[va_ind])\n",
    "                else:\n",
    "                    test_score_fold = scorer(model_current.predict(X[va_ind]), y[va_ind])\n",
    "            else:\n",
    "                test_score_fold       = model_current.score(X[va_ind], y[va_ind])\n",
    "            \n",
    "            cv_results_[fold_str] = np.append(cv_results_[fold_str], test_score_fold) \n",
    "            \n",
    "            line_for_log = name + f\" --> test_score={test_score_fold}\"\n",
    "            cvfit_log_file.write(line_for_log + \"\\n\")\n",
    "            \n",
    "            if save_models:\n",
    "                file_path = os.path.join(path_folder, name) \n",
    "                fileName = Path(file_path)\n",
    "            \n",
    "                if fileName.is_file():\n",
    "                    print(\"WARNING: An exact model with the same hyperparameters is trying to be saved\")\n",
    "                    print(f\"file {file_path} already exists\")\n",
    "                    return None\n",
    "                else:\n",
    "                    joblib.dump(model_current, file_path) \n",
    "            \n",
    "               \n",
    "            if verbose ==1:\n",
    "                print(name,f\" --> test_score={test_score_fold}\")\n",
    "            \n",
    "    cvfit_log_file.close()\n",
    "    test_scores = [cv_results_[f\"split{i}_test_score\"] for i in range(4)]\n",
    "    test_scores_arr = np.array(test_scores)\n",
    "    \n",
    "    #cv_results_[\"best_test_score\"] = np.max(test_scores_arr)\n",
    "    cv_results_[\"mean_test_score\"] = test_scores_arr.mean(axis=0)\n",
    "    cv_results_[\"std_test_score\"] = test_scores_arr.mean(axis=0)\n",
    "\n",
    "    rank = scipy.stats.rankdata(-cv_results_[\"mean_test_score\"], method='ordinal')\n",
    "    cv_results_[\"rank_test_score\"] = rank\n",
    "    # Save in key \"best_params\" the best combination of hyperparams found\n",
    "    # index_best  = np.argmax(rf_grid.cv_results_[\"mean_test_score\"])\n",
    "    # best_params = rf_grid.cv_results_[\"params\"][ind_best]\n",
    "    # cv_results_[\"best_params\"] = best_params\n",
    "\n",
    "    if save_summary:\n",
    "        cv_results_df = pd.DataFrame(cv_results_).sort_values([\"rank_test_score\"])\n",
    "        cv_results_df.to_csv( os.path.join(path_folder, \"cv_results_df.csv\"),index=False)\n",
    "        \n",
    "    return cv_results_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 4\n",
    "rf = sklearn.ensemble.RandomForestRegressor()\n",
    "path_models = \"./saved_models\"\n",
    "cv_results_ = CVfit(X, y, rf, cv, param_grid, scorer=None, \n",
    "                    path_folder=path_models, \n",
    "                    save_models=True, add_time=False, save_summary=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This function returns a dict `cv_results` containing the evaluation metrics of the crossvalidation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we executed with `save_summary=True` we will have `cv_results_df.csv` inside the folder where the models are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_df = pd.read_csv( os.path.join(path_models,\"cv_results_df.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the log of the cvfit and see the information that is printed when verbose=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!cat saved_models/cvfit_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Make a function that given cv_results_df and path_folder...\n",
    "\n",
    "- Loads different models (topK) and creates a \"metamodel\". To do so I need to create a new MetaModel class.\n",
    "\n",
    "    - The metamodel is essentially a list of models: `[m1,m2,m3,...]`\n",
    "    \n",
    "    - The metamodel.predict(X) simply does `np.mean(m1.predict(X), m2.predict(X),....)`\n",
    "    \n",
    "    - The metamodel should have a weighted average prediction method. \n",
    "    \n",
    "          - The `mean_test_score` could be used to assign a weight into the average\n",
    "          - The `std_test_score` could be used to assign a confidence of the prediction of the metamodel\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models will be saved in `path_folder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = os.listdir(path_models)\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load a model from disk using **`joblib.load`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(os.path.join(path_models, models[0]))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Finding indices with `GroupKFold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "groups = np.array([0, 0, 5, 5])\n",
    "group_kfold = GroupKFold(n_splits=2)\n",
    "group_kfold.get_n_splits(X, y, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupKFold(n_splits=2)\n"
     ]
    }
   ],
   "source": [
    "print(group_kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [0 1] TEST: [2 3]\n",
      "[[1 2]\n",
      " [3 4]] [[5 6]\n",
      " [7 8]] [1 2] [3 4]\n",
      "\n",
      "\n",
      "TRAIN: [2 3] TEST: [0 1]\n",
      "[[5 6]\n",
      " [7 8]] [[1 2]\n",
      " [3 4]] [3 4] [1 2]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in group_kfold.split(X, y, groups):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(X_train, X_test, y_train, y_test)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import neural_network\n",
    "\n",
    "cv   = sklearn.model_selection.LeaveOneGroupOut()\n",
    "clf  = sklearn.linear_model.LinearRegression()\n",
    "grid = sklearn.model_selection.GridSearchCV(clf, param_grid={}, cv=cv, \n",
    "                                            return_train_score=True, iid=True, n_jobs=-1)\n",
    "\n",
    "values_in_col8 = np.array(X_df[8].values, dtype=\"int\")\n",
    "\n",
    "grid.fit(X,y, groups=values_in_col8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
