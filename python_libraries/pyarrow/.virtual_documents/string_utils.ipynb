





import string
import random


sentences = ['the big cat went to the bar', 
             'the cat liked the big piano', 
             'the big fish never eats the bigger fish']





%%timeit
get_indices_sentences_matching_words_regex(words=['big', 'cat'], sentences=sentences)





%%timeit
get_indices_sentences_matching_words_regex_parallel(words=['big', 'cat'], sentences=sentences, n_jobs=3)





pool_characters = string.ascii_lowercase

def create_random_word(min_chars=2, max_chars=15, pool_characters=None):
    n_chars = random.randint(min_chars,max_chars)
    return ''.join((random.choice(pool_characters) for x in range(n_chars)))

def create_random_sentence(n_words,pool_characters):
    return ' '.join( [create_random_word(pool_characters=pool_characters) for word in range(n_words) ])



create_random_word(pool_characters= pool_characters)


create_random_sentence(10,pool_characters)


words = 'hi' 


%%time
n_sentences = 2_000_000
n_words = 7
corpus = [create_random_sentence(7, pool_characters) for x in range(n_sentences)]


corpus[0:10]


corpus_14million = 7*[*corpus]


%%time
get_indices_sentences_matching_words_regex(words=['big'], sentences=corpus_14million);


res = get_indices_sentences_matching_words_regex(words=['big'], sentences=corpus_14million);


res_par = get_indices_sentences_matching_words_regex_parallel(words=['big'], sentences=corpus_14million);


%%time
get_indices_sentences_matching_words_regex_parallel(words=['big'], sentences=corpus_14million);


res == res_par





import pyarrow


pyarrow.set_cpu_count(1)


from pyarrow.compute import extract_regex


import re
pattern = re.compile(r'\bbig\b', re.IGNORECASE)


%%time
res = extract_regex(corpus_14million, pattern=r'\bbig\b/i')


%%time
res_arrow = extract_regex(corpus_14million, pattern=r'\bbig\b')
res_arrow_np = np.array(res_arrow.is_valid())


res_arrow_np.sum()


len(res_par)


S = ["Big said the man", 'big boy','bigger than I want']
res_arrow = extract_regex(S,
                          pattern=r'(?i)\bbig\b')


res_arrow


get_indices_sentences_matching_words_regex(words=['big'], sentences=S )


#np.where(res_arrow_np)[0]





corpus_14million











%timeit pyarrow.compute.sum(res_arrow.is_valid())


res_par = get_indices_sentences_matching_words_regex_parallel(words=['big'], sentences=corpus_14million);


len(res_par)


#corpus_14million


res = extract_regex(corpus_14million, pattern=r'big')


len(res)





import numpy as np
res_arrow = np.array(res_arrow.is_valid())


res_arrow == np.array(res_par)


len(np.where(res_arrow)[0])


len(res_par)


res_arrow.shape


!open .








corpus_40million = 20*[*corpus]


%%time
res_arrow = extract_regex(corpus_40million,
                          pattern=r'(?i)\bbig\b')





%%time
get_indices_sentences_matching_words_regex(words=['big'], sentences=corpus_40million);


%%time
get_indices_sentences_matching_words_regex_parallel(words=['big'],
                                                    sentences=corpus_40million,
                                                    n_jobs=3);


%%time
get_indices_sentences_matching_words_regex_parallel(words=['big'],
                                                    sentences=corpus_40million,
                                                    n_jobs=6);


%%time
get_indices_sentences_matching_words_regex_parallel(words=['big'],
                                                    sentences=corpus_40million,
                                                    n_jobs=10);


from gaia_science.common_base.dataframe_tools.utils import get_batches


%%time
n_jobs = 8
n_sentences = len(corpus_40million)
n_batch = int(n_sentences/n_jobs)

partial_start_positions = [0] + list(np.cumsum([len(x) for x in get_batches(range(n_sentences), n_batch)]))





from gaia_science.common_base.string_utils.string_distances.edit_distance import EditDistanceInt


d = EditDistanceInt(1,1,1)
d.evaluate('hi','ho')





from gaia_science.common_base.string_utils.data_structures.bktree import BKTree


import random
import string

def build_vocab(n_words, n_min_per_word, n_max_per_word):
    letters = string.ascii_lowercase
    random.seed(10)
    all_words = []
    for w in range(n_words):
        n = random.randint(n_min_per_word, n_max_per_word)
        all_words.append(''.join(random.choice(letters) for i in range(n)))
                         
    return all_words


vocabulary = build_vocab(500_000, 2, 18)


query = 'help'
max_dist = 1


t1 = %timeit -o  candidates = [w for w in vocabulary if d.evaluate(query,w) <=max_dist]


candidates.sort()
candidates


bktree = BKTree(d.evaluate)


%%time
bktree.fit(vocabulary)


t2 = %timeit -o candidates = bktree.query(query, max_dist=max_dist)


candidates.sort()
candidates


t1.average/t2.average














import pandas as pd


results = pd.DataFrame()


partial = pd.DataFrame({'a':[1,2,3],'b':['a','c','d']})


partial


results = pd.concat((results,partial))


results


results = pd.concat((results,partial))


results.reset_index(drop=True)


def append_many_df(initial_df, n=10_000):
    df_res = pd.DataFrame()
    for i in range(n):
        df_res = pd.concat((df_res, initial_df))
    return df_res.reset_index(drop=True)


initial_df = pd.DataFrame({'a':[1,2,3],'b':['a','c','d']})


%%time
append_many_df(initial_df)


def append_many_df_as_list(initial_df, n=10_000):
    df_res = pd.DataFrame()
    for i in range(n):
        df_res = pd.concat((df_res, initial_df))
    return df_res.reset_index(drop=True)


%%
append_many_df(initial_df)






